name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

env:
  PYTHON_VERSION: '3.11'

jobs:
  lint:
    name: Code Linting
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install ruff==0.1.14
    
    - name: Run Ruff linter
      run: |
        ruff check src/ tests/ --output-format=github
    
    - name: Run Ruff formatter check
      run: |
        ruff format --check src/ tests/

  test:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: lint
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run unit tests
      run: |
        pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=term
      env:
        ENVIRONMENT: test
        USE_MOCK_MODEL: true
        OPENAI_API_KEY: mock-key-for-testing
    
    - name: Upload coverage reports
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
      continue-on-error: true

  llm-judge-evaluation:
    name: LLM-as-a-Judge Quality Gate
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run LLM-as-a-Judge evaluation
      run: |
        python tests/evaluation/evaluate_agent.py
      env:
        ENVIRONMENT: test
        USE_MOCK_MODEL: true
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || 'mock-key' }}
        LOG_LEVEL: INFO
    
    - name: Upload evaluation results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: evaluation-results
        path: evaluation_results.json
        retention-days: 30

  build:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: llm-judge-evaluation
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./Dockerfile
        push: false
        tags: marketing-agent:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
    
    - name: Test Docker image
      run: |
        docker build -t marketing-agent:test .
        docker run --rm marketing-agent:test python -c "import src.agents.marketing.api; print('âœ… Image build successful')"

  integration-test:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: build
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Start services with Docker Compose
      run: |
        docker compose up -d
        sleep 10
    
    - name: Wait for API to be healthy
      run: |
        timeout 60 bash -c 'until curl -f http://localhost:8000/health; do sleep 2; done'
    
    - name: Test API endpoints
      run: |
        # Test health endpoint
        curl -f http://localhost:8000/health
        
        # Test generate endpoint
        curl -X POST http://localhost:8000/generate \
          -H "Content-Type: application/json" \
          -d '{"topic": "artificial intelligence", "tone": "professional"}' \
          -o response.json
        
        # Verify response
        cat response.json
        
        # Test history endpoint
        curl -f http://localhost:8000/history
    
    - name: Check logs
      if: always()
      run: |
        docker compose logs
    
    - name: Stop services
      if: always()
      run: |
        docker compose down -v

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install safety
      run: |
        pip install safety
    
    - name: Run safety check
      run: |
        safety check --file requirements.txt --output text
      continue-on-error: true

  deploy:
    name: Deploy (Placeholder)
    runs-on: ubuntu-latest
    needs: [integration-test, security-scan]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Deploy placeholder
      run: |
        echo "ðŸš€ Deployment step placeholder"
        echo "In production, this would deploy to:"
        echo "  - Kubernetes cluster"
        echo "  - Cloud provider (AWS/GCP/Azure)"
        echo "  - Container registry"
        echo ""
        echo "âœ… Pipeline completed successfully!"

